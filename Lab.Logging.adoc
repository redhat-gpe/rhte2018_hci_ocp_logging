:opencf: link:https://labs.opentlc.com/[OPENTLC lab portal^]
:account_management: link:https://www.opentlc.com/account/[OPENTLC Account Management page^]
:quay_hostname: quay.rhte.example.opentlc.com
:cluster_base: REPL.rhte.opentlc.com
:cluster_master: master.{cluster_base}
:bastion: bastion.{cluster_base}
:kibana: https://kibana.apps.{cluster_base}
:application1: app1
:application2: app2
:oc_version: 3.10.14
:oc_download_location: https://mirror.openshift.com/pub/openshift-v3/clients/{oc_version}

= Logging OpenShift Lab

== Lab Overview

Welcome to the RHTE Logging for Hybrid Cloud lab. This lab is based on the Aggregated Logging feature available on OpenShift Container Platform (OCP). This Aggregated Logging is based on the popular Elasticsearch, Fluentd, Kibana (EFK) stack.

The Aggregated Logging feature aggregates node (system), Kubernetes events, and pod logs and saves it for search, analysis and dashboards.

In the lab, expect to learn how to install, and upgrade the aggregated Logging Stack and use the powerful features of Kibana to visualize logs for troubleshooting and for dashboards.

== Lab Setup

=== Configure the OpenShift Command Line Interface

In this lab, you use the command line for some of the tasks. The lab assumes that you are using your laptop and a local OpenShift command line utility.

You will need version {oc_version} of the OpenShift command line utility. If you already have the OpenShift CLI installed you can check the version installed:

[source,bash]
----
oc version
----

.Sample Output
[source,text]
----
oc v3.10.14
kubernetes v1.10.0+b81c8f8
features: Basic-Auth
----

Make sure the the version displayed matches {oc_version}.

If it does not you can download the correct version of the OpenShift Command Line interface from {oc_download_location}. In this directory you will find a `linux`, `windows` and `macosx` subdirectory. Navigate to the directory for your operating system and download the OpenShift command line utility (oc.tar.gz for Linux and MacOS or oc.zip for Windows). You will need need to unarchive the downloaded file and then move the "oc" binary into a directory in your PATH (usually $HOME/bin).

Then verify again that the version displayed is corect.


=== These are your lab end points 


cluster_base: {cluster_base}

cluster_master: {master_base}

bastion: {bastion}

kibana: {kibana}

Where REPL = Lab Attendees GUID

=== Connect to the OpenShift Cluster via oc

oc 

Make sure you can log in to the OpenShift cluster.

[NOTE]
The cluster URL is https://{cluster_master}.

. Use the `oc login` command to log in to the cluster, making sure to replace the URLs with your cluster's URLs:
+
[source,text]
----
oc login -u user1 -p r3dh4t1! <CLUSTER_URL>
----
+
[NOTE]
If you see a warning about an insecure certificate, type `y` to connect insecurely.
+
.Sample Output
[source,text]
----
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>
----

=== Connect to the OpenShift Web Console

Make sure you can log in to the OpenShift cluster as user2.

[NOTE]
The cluster URL is https://{cluster_master}.

. Use your web browser to log in to the cluster https://{cluster_master}.
. At the login prompt use username `user2` and password `r3dh4t1!`

=== Connect to the bastion host via ssh

Connect to the {bastion} host via SSH using the private key for OpenTLC:
[source,bash]
----
ssh -i <Your OPENTLC private key> <Your OPENTLC UserID>@<BASTION_HOST>
----

Once you're on the bastion host, become root:
[source,bash]
----
sudo -i
----

Check that you're `system:admin` on your OpenShift cluster:
[source,bash]
----
oc whoami
----

You should see

.Output
[source,text]
----
system:admin
----

OpenShift cluster has pre-created users. We will be using `user1`, `user2` and `user3`
Add `cluster-admin` privileges to the `user3`:
[source,bash]
----
oc adm policy add-cluster-role-to-user cluster-admin user3
----
You should see

.Output
[source,text]
----
cluster role "cluster-admin" added: "user3"
----

== Create applications in the OpenShift Cluster

In this section you are creating 2 applications in the cluster under 2 different users. These applications will run in separate namespaces and produce various logs. In the real world this might be end-user's applications.

=== Create application as user1 via command line

. Log into the OpenShift Cluster under user1 (use https://{cluster_master} as `<CLUSTER_URL>`).
+
[source,bash]
----
oc login -u user1 -p r3dh4t1! <CLUSTER_URL>
----
+
. Create a project for Jenkins
+
[source,bash]
----
oc new-project lab1 --display-name "First Log producer"
----
+
. You will be building a (very) simple python application.


=== Create application as user2 via Web Console

. Open your web browser and log into the OpenShift Web Console under user2 (use https://{cluster_master} ).
. Click `Create Project` to create new project for the sample application
+
Use the Name `lab2` and Display Name `Second Sample application`
+
. After the project is created click on `Second Sample application` in the list of `My Projects` to select it
+
The project is empty at the moment. OpenShift comes with a pre-configured PHP/MySQL pod to use. We'll use it as an example to produce some logs.
+
. Open `Browse Catalog` and select `CakePHP + MySQL (ephemeral)`
+
Click `next` on the 1st page of application deployment wizard `Information` page, click `create` on the 2nd `Configuration` page(we don't need to change any configuration).

=== Check that the application logs are flowing in

In order to view application logs you will need to log in to Kibana as a regular user.

. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user1` and password `r3dh4t1!`
+
[NOTE]
If you already logged in as a different user you may want to use another browser or incognito browsing window.
+
. In the `Discovery` tab of Kibana add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Verify that logs from the application deployed as `user1` can be seen and no logs from namespace created as `user2` can be seen

Validated at this point:
Regular user can only see logs from his own application.

=== Check that the system logs are flowing in

In order to view system logs you should log in to Kibana as a user with cluster-admin privileges:

. Close the old incognito browsing window, so that old cookies don't interfere with the new login.
. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user3` and password `r3dh4t1!`
+
[NOTE]
user3 was given cluster-admin privileges previously.
+
. In the `Discovery` tab of Kibana switch to `projects.*` index pattern
. Then add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Verify that logs from the application deployed as `user1` can be seen as well as logs from namespace created as `user2`
. In the `Discovery` tab of Kibana switch to `.operations.*` index pattern
. Verify that system logs from hosts can be seen

Validated at this point:
Cluster admins can see logs from all namespaces
Cluster admins can see logs from hosts and administrative namespaces

== Upgrade the Logging setup

In this section you will upgrade the logging bits to logging from OpenShift version 3.11.

* SSH to the bastion host.
* Become root on the bastion host
[source,bash]
----
sudo -i
----
* Check out the repository that contains logging bits from OKD 3.11 (snapshot of openshift-ansible repo):
[source,bash]
----
cd /root/
git clone https://github.com/t0ffel/openshift-ansible
----
* Edit the inventory file:
[source,bash]
----
vim /etc/ansible/hosts
----
* Add the following lines to the inventory file in `[OSEv3:vars]` section right after `Enable cluster logging`.
[source,bash]
----
openshift_logging_image_prefix="openshift/origin-"
openshift_logging_elasticsearch_image="openshift/origin-logging-elasticsearch5"
openshift_logging_kibana_image="openshift/origin-logging-kibana5"
openshift_logging_fluentd_image="openshift/origin-logging-fluentd"
openshift_logging_es_allow_external=True
openshift_logging_elasticsearch_proxy_image=openshift/oauth-proxy:v1.1.0
openshift_logging_kibana_proxy_image=openshift/oauth-proxy:v1.1.0
----
This directs the installer to use OKD images of logging components from v3.11 and allow external route to Elasticsearch cluster.

Enable cluster logging section should look as follows:
[source,bash]
----
# Enable cluster logging
########################
openshift_logging_install_logging=True
openshift_logging_image_prefix="openshift/origin-"
openshift_logging_elasticsearch_image="openshift/origin-logging-elasticsearch5"
openshift_logging_kibana_image="openshift/origin-logging-kibana5"
openshift_logging_fluentd_image="openshift/origin-logging-fluentd"
openshift_logging_es_allow_external=True
openshift_logging_elasticsearch_proxy_image=openshift/oauth-proxy:v1.1.0
openshift_logging_kibana_proxy_image=openshift/oauth-proxy:v1.1.0

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/srv/nfs
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
openshift_logging_storage_labels={'storage': 'logging'}
openshift_logging_es_pvc_storage_class_name=''

openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}

openshift_logging_es_cluster_size=1
openshift_logging_curator_default_days=3
----
* Make sure you're in the `/root/openshift-ansible` directory:
[source, bash]
----
pwd
----
* Execute ansible-playbook to upgrade the logging bits:
[source,bash]
----
ansible-playbook -vvv --become --become-user root \
  --inventory /etc/ansible/hosts \
  -e deployment_type=origin \
  -e openshift_image_tag=v3.11 \
  -e openshift_pkg_version="-3.11*" \
  playbooks/openshift-logging/config.yml
----

[Note]
Execution of the playbook may take few minutes.

This instructs the installer to deploy logging bits from v3.11

Ansible playbook should finish running without any errors.

.Sample Output
[source,text]
----
PLAY RECAP *************************************************************************************************************************************************************************
infranode1.4b43.internal   : ok=2    changed=1    unreachable=0    failed=0
localhost                  : ok=12   changed=0    unreachable=0    failed=0
master1.4b43.internal      : ok=303  changed=58   unreachable=0    failed=0
node1.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
node2.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
support1.4b43.internal     : ok=1    changed=0    unreachable=0    failed=0


INSTALLER STATUS *******************************************************************************************************************************************************************
Initialization   : Complete (0:00:10)
Logging Install  : Complete (0:02:58)
----

Validate that the logging stack was updated:

. Navigate to `openshift-logging` namespace and check the running pods
[source,bash]
----
oc project openshift-logging
oc get po
----

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc project openshift-logging
Now using project "openshift-logging" on server "https://master.4b43.openshift.opentlc.com:443".
[root@bastion openshift-ansible]# oc get po
NAME                                      READY     STATUS    RESTARTS   AGE
logging-es-data-master-7m1ej4aj-6-kppvt   2/2       Running   0          1m
logging-fluentd-5xdsq                     1/1       Running   0          1m
logging-fluentd-65hwq                     1/1       Running   0          1m
logging-fluentd-9bndh                     1/1       Running   0          1m
logging-fluentd-wf6hc                     1/1       Running   0          3h
logging-kibana-2-mnhm7                    2/2       Running   0          1m
[root@bastion openshift-ansible]#
----
Age of the Elasticsearch pod should be new - it was just spun up by the installer.

It should be using image `openshift/origin-logging-elasticsearch5`. You can check it for example if you do `oc describe pod <POD NAME>`

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc describe po logging-es-data-master-7m1ej4aj-6-kppvt
Name:           logging-es-data-master-7m1ej4aj-6-kppvt
Namespace:      openshift-logging
Node:           infranode1.4b43.internal/192.199.0.91
Start Time:     Fri, 24 Aug 2018 17:26:35 +0000
Labels:         component=es
                deployment=logging-es-data-master-7m1ej4aj-6
                deploymentconfig=logging-es-data-master-7m1ej4aj
                logging-infra=elasticsearch
                provider=openshift
Annotations:    openshift.io/deployment-config.latest-version=6
                openshift.io/deployment-config.name=logging-es-data-master-7m1ej4aj
                openshift.io/deployment.name=logging-es-data-master-7m1ej4aj-6
                openshift.io/scc=restricted
Status:         Running
IP:             10.1.2.38
Controlled By:  ReplicationController/logging-es-data-master-7m1ej4aj-6
Containers:
  elasticsearch:
    Container ID:   docker://8fee48f81997355bb03dea77d692605d367c799b25bdb950ce4fdacfa4bac453
    Image:          openshift/origin-logging-elasticsearch5
    Image ID:       docker-pullable://docker.io/openshift/origin-logging-elasticsearch5@sha256:5151753acbc1d55459223cda392cead1adfb53c6d9e4955b39d4cca5536faa58
    Ports:          9200/TCP, 9300/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Fri, 24 Aug 2018 17:26:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  16Gi
    Requests:
      cpu:      1
      memory:   16Gi
    Readiness:  exec [/usr/share/elasticsearch/probe/readiness.sh] delay=10s timeout=30s period=5s #success=1 #failure=3
    Environment:
      DC_NAME:                  logging-es-data-master-7m1ej4aj
      NAMESPACE:                openshift-logging (v1:metadata.namespace)
      KUBERNETES_TRUST_CERT:    true
      SERVICE_DNS:              logging-es-cluster
      CLUSTER_NAME:             logging-es
      INSTANCE_RAM:             16Gi
      HEAP_DUMP_LOCATION:       /elasticsearch/persistent/heapdump.hprof
      NODE_QUORUM:              1
      RECOVER_EXPECTED_NODES:   1
      RECOVER_AFTER_TIME:       5m
      READINESS_PROBE_TIMEOUT:  30
      POD_LABEL:                component=es
      IS_MASTER:                true
      HAS_DATA:                 true
      PROMETHEUS_USER:          system:serviceaccount:openshift-metrics:prometheus
      PRIMARY_SHARDS:           1
      REPLICA_SHARDS:           0
    Mounts:
      /elasticsearch/persistent from elasticsearch-storage (rw)
      /etc/elasticsearch/secret from elasticsearch (ro)
      /etc/podinfo from podinfo (ro)
      /usr/share/java/elasticsearch/config from elasticsearch-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from aggregated-logging-elasticsearch-token-q7v4t (ro)
  proxy:
    Container ID:  docker://cdb5626aab755615c2dfd2b7287cb266d7d4ef6f8c09e663127ea188f73c4830
    Image:         openshift/oauth-proxy:v1.1.0
    Image ID:      docker-pullable://docker.io/openshift/oauth-proxy@sha256:731c1fdad1de4bf68ae9eece5e99519f063fd8d9990da312082b4c995c4e4e33
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --upstream-ca=/etc/elasticsearch/secret/admin-ca
      --https-address=:4443
      -provider=openshift
      -client-id=system:serviceaccount:openshift-logging:aggregated-logging-elasticsearch
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret=ekdMM0pXbkJwbExTOEJHQg==
      -basic-auth-password=66l71kZ55ccghoTC
      -upstream=https://localhost:9200
      -openshift-sar={"namespace": "openshift-logging", "verb": "view", "resource": "prometheus", "group": "metrics.openshift.io"}
      -openshift-delegate-urls={"/": {"resource": "prometheus", "verb": "view", "group": "metrics.openshift.io", "namespace": "openshift-logging"}}
      --tls-cert=/etc/tls/private/tls.crt
      --tls-key=/etc/tls/private/tls.key
      -pass-access-token
      -pass-user-headers
    State:          Running
      Started:      Fri, 24 Aug 2018 17:26:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  64Mi
    Requests:
      cpu:        100m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /etc/elasticsearch/secret from elasticsearch (ro)
      /etc/tls/private from proxy-tls (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from aggregated-logging-elasticsearch-token-q7v4t (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  proxy-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-tls
    Optional:    false
  elasticsearch:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  logging-elasticsearch
    Optional:    false
  elasticsearch-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      logging-elasticsearch
    Optional:  false
  podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      limits.memory -> mem_limit
  elasticsearch-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  logging-es-0
    ReadOnly:   false
  aggregated-logging-elasticsearch-token-q7v4t:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  aggregated-logging-elasticsearch-token-q7v4t
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/infra=true
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type    Reason     Age   From                               Message
  ----    ------     ----  ----                               -------
  Normal  Scheduled  2m    default-scheduler                  Successfully assigned logging-es-data-master-7m1ej4aj-6-kppvt to infranode1.4b43.internal
  Normal  Pulled     2m    kubelet, infranode1.4b43.internal  Container image "openshift/origin-logging-elasticsearch5" already present on machine
  Normal  Created    2m    kubelet, infranode1.4b43.internal  Created container
  Normal  Started    2m    kubelet, infranode1.4b43.internal  Started container
  Normal  Pulled     2m    kubelet, infranode1.4b43.internal  Container image "openshift/oauth-proxy:v1.1.0" already present on machine
  Normal  Created    2m    kubelet, infranode1.4b43.internal  Created container
  Normal  Started    2m    kubelet, infranode1.4b43.internal  Started container
----


== Create Kibana Dashboards

. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user3` and password `r3dh4t1!`
+
[NOTE]
user3 was given cluster-admin privileges previously.


=== Create Saved Searches

. In the `Discovery` tab of Kibana switch to `projects.*` index pattern
. Then add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Click `Save` and save the search under name `pod logs`
. In the `Discovery` tab of Kibana switch to `.operations.*` index pattern
. Remove all existing columns and add columns for:
.. `hostname`
.. `systemd.t.COMM`
.. `message`
. Click `Save`
.. Enter name `ops logs`
.. Check checkbox 'Save as a new search'
.. Click `Save`

=== Create Visualizations
Fist visualization we're going to create is the pie chart visualization of Namespace by volume of logs.

. Click Visualizations tab
.. Click `+`
.. Choose `Pie`
.. In `From a New Search, Select Index` select `project.*`
.. Under `buckets` -> `Select buckets type` choose `Split slices`
.. Under `Aggregation` select `Term` in drop-down box
.. Under `Field` scroll down and select `kubernetes.namespace_name`
.. Hit 'run' triangle button to see the preview
.. Click `Save`
.. Enter name `namespaces by log volume`
.. Click `Save`

Next visualization we're going to create is a histogram for ops logs/hosts.

. Click Visualizations tab
.. Click `+`
.. Choose `Vertical Bar`
.. In `Or, From a Saved Search` select `ops logs`
+
[NOTE]
We're re-using `ops logs` saved search which we created on an earlier step.
+
.. Under `buckets` -> `Select buckets type` choose `X-Axis`
.. Under `Aggregation` select `Date Histogram` in drop-down box
.. hit 'run' button to preview the histogram
.. Click `Add sub-buckets` below `buckets`->`X-Axis`
.. Under `Select buckets type` choose `Split series`
.. In `Sub Aggregation` drop-down select `Term`
.. In `Field` choose `hostname`
.. Click `Save`
.. Enter name `ops logs histogram by hostname`
.. Click `Save`

TODO:  - append pie chart for namespaces with pod names

=== Create Dashboard
We will create a dashboard that will display all the visualizations and saved searches created on previous steps.

. Click Dashboard tab
.. Click `+`
.. Click `add`
.. Add all visualizations from `Visualization` tab
.. Add all saved searches from `Search tab`
.. Drag and resize visualizations and searches so that they fit the screen nicely.
.. Click `Save`
.. Enter name `Main dashboard`
.. Click `Save`

== Eventrouter & Timelion

=== Deploy Eventrouter component

Eventrouter is the component of Logging stack that allows collecting Kubernetes events.

* SSH to the bastion host.
* Become root on the bastion host
[source,bash]
----
sudo -i
----
* Edit the inventory file:
[source,bash]
----
vim /etc/ansible/hosts
----
* Add the following lines to the inventory file in `[OSEv3:vars]` section right after `Enable cluster logging`.
[source,bash]
----
openshift_logging_install_eventrouter=True
openshift_logging_eventrouter_image="openshift/origin-logging-eventrouter"
----
This directs the installer to add OKD's eventrouter logging component from v3.11.

Enable cluster logging section should look as follows:
[source,bash]
----
TODO
----

* Make sure you're in the `/root/openshift-ansible` directory:
[source, bash]
----
cd /root/openshift-ansible
----
* Execute ansible-playbook to deploy the Eventrouter component:
[source,bash]
----
ansible-playbook -vvv --become --become-user root \
  --inventory /etc/ansible/hosts \
  -e deployment_type=origin \
  -e openshift_image_tag=v3.11 \
  -e openshift_pkg_version="-3.11*" \
  playbooks/openshift-logging/config.yml
----

[Note]
Execution of the playbook may take few minutes.

This instructs the installer to deploy Eventrouter component from OKD v3.11

Ansible playbook should finish running without any errors.

.Sample Output
[source,text]
----
PLAY RECAP *************************************************************************************************************************************************************************
infranode1.4b43.internal   : ok=2    changed=1    unreachable=0    failed=0
localhost                  : ok=12   changed=0    unreachable=0    failed=0
master1.4b43.internal      : ok=303  changed=58   unreachable=0    failed=0
node1.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
node2.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
support1.4b43.internal     : ok=1    changed=0    unreachable=0    failed=0


INSTALLER STATUS *******************************************************************************************************************************************************************
Initialization   : Complete (0:00:10)
Logging Install  : Complete (0:02:58)
----

Validate that the Eventrouter component was deployed successfully:

. Navigate to `default` namespace and check the running pods
[source,bash]
----
oc project default
oc get po
----

[NOTE]
Eventrouter is deployed to `default` namespace, not to `openshift-logging` namespace.

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc project default
Now using project "default" on server "https://master.4b43.openshift.opentlc.com:443".
[root@bastion openshift-ansible]# oc get po
TODO
[root@bastion openshift-ansible]#
----
Age of the Eventrouter pod should be new - it was just spun up by the installer.

It should be using image `openshift/origin-logging-eventrouter`. You can check it for example if you do `oc describe pod <POD NAME>`

=== View Kuberentes events in Kibana


=== Correlate events via Timelion

In this section you will learn how to use Timelion feature of Kibana. This is a new Kibana application added in Kibana 5.x (included in OpenShift v3.11).

- Delete image stream and validate that it is reflected in the Dashboard

== Forwarding logs to different cluster

=== Get the certificate from destination cluster

TODO

=== Change fluentd config on the source cluster

TODO

== Cleanup

Now that you have finished this lab, please take a moment to clean up the environment for the next set of students.

. Log into the OpenShift Cluster 1 (https://{cluster_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Jenkins Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-jenkins
----
+
. Delete the Development Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app-dev
----
+
. Delete the Production Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----
+
. Log into the OpenShift Cluster 2 (https://{cluster2_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Production Project on Cluster 2
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----

Congratulations! You finished this lab.
